{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mini VLA-Agent: Vision + Language â†’ Action"
      ],
      "metadata": {
        "id": "cfb9wIozU0ih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JWUdTuJGTvaW"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium imageio[ffmpeg] transformers torch torchvision accelerate sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from transformers import CLIPProcessor, CLIPModel, DistilBertModel, DistilBertTokenizer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2 as imageio\n",
        "from IPython.display import Video, HTML\n",
        "import json\n",
        "import imageio\n",
        "from base64 import b64encode\n"
      ],
      "metadata": {
        "id": "s3CccDq-T7OR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniVLAAgent(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.fuse = nn.Linear(self.clip.config.projection_dim + self.text_encoder.config.dim, 256)\n",
        "        self.policy_head = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, image, text_input):\n",
        "        with torch.no_grad():\n",
        "            vision_emb = self.clip.get_image_features(**image)\n",
        "            text_emb = self.text_encoder(**text_input).last_hidden_state[:, 0, :]\n",
        "        fused = torch.relu(self.fuse(torch.cat([vision_emb, text_emb], dim=-1)))\n",
        "        return self.policy_head(fused)\n"
      ],
      "metadata": {
        "id": "ERN2O9tQU2yP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "p571scgTVIU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "n_actions = env.action_space.n\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "agent = MiniVLAAgent(n_actions).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "text_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSg1djFZU-zi",
        "outputId": "b44f9510-a362-4f47-ebbb-38689148e55c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vla_action(frame, instruction):\n",
        "    image = clip_processor(images=Image.fromarray(frame), return_tensors=\"pt\").to(device)\n",
        "    text_input = text_tokenizer(instruction, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = agent(image, text_input)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        action = torch.argmax(probs, dim=-1).item()\n",
        "    return action"
      ],
      "metadata": {
        "id": "Gz-SDz_4VNaQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "frames = []\n",
        "logs = []  # store logs for analysis\n",
        "\n",
        "obs, _ = env.reset()\n",
        "instruction = \"Keep the pole balanced\"\n",
        "print(f\"ðŸŽ¯ Instruction for VLA: '{instruction}'\\n\")\n",
        "\n",
        "for step in range(1000):\n",
        "    frame = env.render()\n",
        "    action = vla_action(frame, instruction)  # vision-language reasoning\n",
        "\n",
        "    obs, reward, done, trunc, info = env.step(action)\n",
        "    frames.append(frame)\n",
        "\n",
        "    # --- Logging for understanding ---\n",
        "    print(f\"ðŸŒ€ Step {step}\")\n",
        "    print(f\"Observation (simplified): {str(obs)[:120]}...\")\n",
        "    print(f\"Action taken: {action}\")\n",
        "    print(f\"Reward received: {reward:.3f}\")\n",
        "    if 'clip_score' in info:\n",
        "        print(f\"CLIP similarity score: {info['clip_score']:.3f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # --- Save to structured log list ---\n",
        "    logs.append({\n",
        "        \"step\": step,\n",
        "        \"instruction\": instruction,\n",
        "        \"action\": str(action),\n",
        "        \"reward\": float(reward),\n",
        "        \"done\": done,\n",
        "        \"truncated\": trunc,\n",
        "        \"info\": {k: float(v) if isinstance(v, (int, float)) else str(v) for k, v in info.items()}\n",
        "    })\n",
        "\n",
        "    if done or trunc:\n",
        "        print(\"\\nâœ… Task completed or terminated early.\")\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "# --- Optional: export log file for GitHub transparency ---\n",
        "with open(\"vla_run_log.json\", \"w\") as f:\n",
        "    json.dump(logs, f, indent=4)\n",
        "\n",
        "print(\"\\nðŸ“„ Saved structured log to 'vla_run_log.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0VcUEIGWFhk",
        "outputId": "6459424d-44ba-46cf-8f7f-d52bced299a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Instruction for VLA: 'Keep the pole balanced'\n",
            "\n",
            "ðŸŒ€ Step 0\n",
            "Observation (simplified): [ 0.02522683  0.20051472  0.04357047 -0.27844906]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 1\n",
            "Observation (simplified): [ 0.02923713  0.3949889   0.03800149 -0.5570778 ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 2\n",
            "Observation (simplified): [ 0.0371369   0.58955735  0.02685993 -0.83755   ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 3\n",
            "Observation (simplified): [ 0.04892805  0.78430235  0.01010893 -1.1216663 ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 4\n",
            "Observation (simplified): [ 0.06461409  0.9792903  -0.01232439 -1.4111613 ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 5\n",
            "Observation (simplified): [ 0.08419991  1.1745628  -0.04054762 -1.7076712 ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 6\n",
            "Observation (simplified): [ 0.10769116  1.3701268  -0.07470104 -2.012694  ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 7\n",
            "Observation (simplified): [ 0.1350937   1.5659413  -0.11495492 -2.327538  ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 8\n",
            "Observation (simplified): [ 0.16641253  1.7619026  -0.16150568 -2.6532621 ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "ðŸŒ€ Step 9\n",
            "Observation (simplified): [ 0.20165057  1.9578255  -0.21457092 -2.9905987 ]...\n",
            "Action taken: 1\n",
            "Reward received: 1.000\n",
            "------------------------------------------------------------\n",
            "\n",
            "âœ… Task completed or terminated early.\n",
            "\n",
            "ðŸ“„ Saved structured log to 'vla_run_log.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Save frames as video ---\n",
        "video_path = \"vla_cartpole_run.mp4\"\n",
        "imageio.mimsave(video_path, frames, fps=30)\n",
        "print(f\"ðŸŽ¬ Saved video to: {video_path}\")\n",
        "\n",
        "# --- Display inline in Colab ---\n",
        "mp4 = open(video_path, 'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=480 controls>\n",
        "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "ZAKD83u5W2Os",
        "outputId": "5b53b675-8676-4b99-c2f8-c44d7a038ea1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¬ Saved video to: vla_cartpole_run.mp4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<video width=480 controls>\n",
              "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "</video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}