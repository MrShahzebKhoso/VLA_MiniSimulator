# ğŸ§  Vision-Language-Action (VLA) Mini Simulation

A simple, Colab-ready **Vision-Language-Action (VLA)** demonstration that connects *vision*, *language*, and *action* â€” inspired by cutting-edge embodied AI research (e.g., RT-X, OpenVLA, AIRoA).

This project demonstrates how a vision-language model can interpret **visual input** from an environment and decide an **action** based on a natural language instruction.

---

## ğŸ¯ Objective

> â€œKeep the pole balanced.â€

Using the *CartPole-v1* environment, the system:

- Observes the visual state of the environment.  
- Receives a natural language instruction.  
- Uses a simple vision-language reasoning method to select an action.  
- Logs and visualizes its step-by-step behavior.

---

## âš™ï¸ Setup and Usage

Run this project easily on **Google Colab** â€” no GPU required.

### Option 1: Open in Colab

| Notebook | Description |
|-----------|--------------|
| [ğŸ”— VLA_Demo_1.ipynb](VLA_Demo_1.ipynb) | Basic reasoning and visualization |
| [ğŸ”— VLA_Demo_2.ipynb](VLA_Demo_2.ipynb) | Enhanced logging and video export |

> Simply open either notebook in Google Colab and run all cells sequentially.

---

## ğŸ§© Project Structure

```
vla-mini-simulation/
â”‚
â”œâ”€â”€ VLA_Demo_1.ipynb           # Basic simulation and reasoning
â”œâ”€â”€ VLA_Demo_2.ipynb           # With enhanced logging and video output
â””â”€â”€ README.md                  # This file
```

---

## ğŸ§ª Example Output

### ğŸ–¥ï¸ Logs

```
ğŸ¯ Instruction: "Keep the pole balanced"

ğŸŒ€ Step 5
Observation: [-0.0361, -1.1556, 0.0478, 1.7100]
Action taken: 0
Reward received: 1.000
------------------------------------------------------------
```

### ğŸ¥ Video (auto-generated)

Each run produces a short `.mp4` showing how the agent balances the pole based on the given instruction.

---

## ğŸ“„ Features

âœ… Vision-language reasoning with CLIP-style embeddings  
âœ… Real-time action and reward logging  
âœ… JSON-based structured logs for analysis  
âœ… Automatic MP4 video generation  
âœ… 100% runnable in Google Colab  

---

## ğŸŒ Research Context

This mini-project reflects the core principles of **embodied AI**, linking perception, reasoning, and motor control.  
It aligns conceptually with projects like:

- AIRoAâ€™s Vision-Language-Action foundation models  
- DeepMind RT-X / RT-2  
- Stanfordâ€™s OpenVLA  

---

## ğŸ‘¨â€ğŸ’» Author

**Shahzeb Khoso**  
AI Research Engineer | Generative & Embodied Intelligence  
ğŸ“ [LinkedIn](https://www.linkedin.com/in/shahzebkhoso) | [GitHub](https://github.com/<yourusername>)

---
